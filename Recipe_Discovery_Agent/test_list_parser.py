#!/usr/bin/env python3
"""
Test file for FireCrawl-based list extraction.

Tests the new strategic FireCrawl extraction method on the problematic Joy to the Food URL.
URL: https://joytothefood.com/breakfast-recipes-with-over-30g-of-protein-without-protein-powder/
"""

import asyncio
import os
from Tools.Tools import extract_list_with_firecrawl

# Test URL that was causing issues with HTML-based parser
TEST_URL = "https://joytothefood.com/breakfast-recipes-with-over-30g-of-protein-without-protein-powder/"

async def test_firecrawl_extraction():
    """Test the new FireCrawl-based list extraction on the problematic URL."""
    
    print("üß™ TESTING FIRECRAWL LIST EXTRACTION")
    print(f"üìç Test URL: {TEST_URL}")
    print("=" * 80)
    
    # Get FireCrawl API key from environment
    firecrawl_key = os.getenv('FIRECRAWL_API_KEY')
    if not firecrawl_key:
        print("‚ùå FIRECRAWL_API_KEY not found in environment variables")
        print("Please set FIRECRAWL_API_KEY before running the test")
        return
    
    try:
        # Test FireCrawl extraction with different limits
        for max_urls in [3, 5, 8]:
            print(f"\nüéØ Testing FireCrawl extraction with max_urls = {max_urls}")
            print("-" * 60)
            
            recipes = await extract_list_with_firecrawl(TEST_URL, firecrawl_key, max_urls)
            
            print(f"üìä Results: Found {len(recipes)} recipe URLs")
            
            if not recipes:
                print("‚ùå NO RECIPES FOUND")
                continue
            
            # Display each found recipe
            for i, recipe in enumerate(recipes, 1):
                print(f"\nüç≥ Recipe #{i}:")
                print(f"   Title: {recipe.get('title', 'No title')}")
                print(f"   URL: {recipe.get('url', 'No URL')}")
                print(f"   Source: {recipe.get('source', 'Unknown')}")
                print(f"   Description: {recipe.get('snippet', 'No description')[:100]}...")
                
                # Validate URL format
                url = recipe.get('url', '')
                if url.startswith('http'):
                    print(f"   ‚úÖ Valid URL format")
                else:
                    print(f"   ‚ùå Invalid URL format")
                
                # Check if it's the expected individual recipe URLs
                expected_urls = [
                    'high-protein-breakfast-quesadilla',
                    'cottage-cheese-egg-bites'
                ]
                
                if any(expected in url for expected in expected_urls):
                    print(f"   ‚úÖ Found expected recipe URL!")
                
                # Check if it incorrectly found category pages
                if '/recipes/' in url and url.endswith('/recipes/'):
                    print(f"   ‚ö†Ô∏è  WARNING: This looks like a category page, not individual recipe")
        
        print(f"\n" + "=" * 80)
        print("üéØ SPECIFIC TEST CASES")
        print("Expected to find:")
        print("  ‚úì https://joytothefood.com/high-protein-breakfast-quesadilla/")
        print("  ‚úì https://joytothefood.com/cottage-cheese-egg-bites/")
        print("Should NOT find:")
        print("  ‚úó https://joytothefood.com/recipes/")
        
    except Exception as e:
        print(f"‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()

def main():
    """Run the test."""
    print("Starting FireCrawl List Extraction Test...")
    asyncio.run(test_firecrawl_extraction())
    print("\n‚úÖ Test completed!")

if __name__ == "__main__":
    main()