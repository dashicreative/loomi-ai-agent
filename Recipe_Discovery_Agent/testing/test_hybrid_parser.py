#!/usr/bin/env python3
"""
Test file for hybrid list extraction approach.

Strategy: Fast HTML scraping + our own LLM extraction to mimic FireCrawl's success
but with dramatically better performance and cost efficiency.

Approach:
1. Fast HTTP scrape to get HTML content (~1-2 seconds)
2. Use OpenAI GPT-3.5-turbo for structured extraction (~3-5 seconds)
3. Total time target: ~5-7 seconds vs FireCrawl's 37+ seconds
4. Cost: ~$0.002 per extraction vs FireCrawl's ~$0.015-0.03
"""

import asyncio
import os
import time
import httpx
import json
from dotenv import load_dotenv

# Load environment variables from parent directory .env file  
import sys
sys.path.append('..')
env_loaded = load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "../.env"))
print(f"üîß Debug: .env loaded = {env_loaded}")
print(f"üîß Debug: OPENAI_API_KEY = {'***' + os.getenv('OPENAI_API_KEY', 'NOT_FOUND')[-4:] if os.getenv('OPENAI_API_KEY') else 'NOT_FOUND'}")

# Test URL that was causing issues with HTML-based parser
TEST_URL = "https://joytothefood.com/breakfast-recipes-with-over-30g-of-protein-without-protein-powder/"

async def extract_list_with_hybrid_approach(url: str, openai_key: str, max_urls: int = 10):
    """
    Hybrid approach: Fast HTML scraping + GPT-3.5 extraction.
    
    Mimics FireCrawl's structured extraction but much faster and cheaper.
    """
    if not openai_key:
        return []
    
    try:
        # Step 1: Fast HTML scraping (1-2 seconds)
        print(f"üì° Step 1: Fast HTML scrape...")
        scrape_start = time.time()
        
        async with httpx.AsyncClient(timeout=15.0) as client:
            response = await client.get(url, follow_redirects=True)
            response.raise_for_status()
            html_content = response.text
            
        scrape_time = time.time() - scrape_start
        print(f"   ‚úÖ HTML scraped in {scrape_time:.2f}s ({len(html_content):,} chars)")
        
        # Step 2: Clean and prepare content for LLM
        # Extract just the main content area to reduce token usage
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script, style, and other non-content elements
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()
        
        print(f"   ‚úÖ Content ready for LLM analysis")
            
        # Get text content with some structure preserved
        clean_content = soup.get_text(separator='\n', strip=True)
        
        # Limit content size to control costs (GPT-3.5 has token limits)
        max_chars = 15000  # ~3750 tokens, leaving room for prompt and response
        if len(clean_content) > max_chars:
            clean_content = clean_content[:max_chars] + "...[content truncated]"
            
        print(f"   ‚úÖ Content cleaned ({len(clean_content):,} chars for LLM)")
        
        # Step 3: LLM extraction with improved guidance
        print(f"ü§ñ Step 2: LLM extraction with improved guidance...")
        llm_start = time.time()
        
        # Clear, focused prompt without URL format constraints
        prompt = f"""You are analyzing a recipe list page. Extract up to {max_urls} individual recipe links.

GUIDANCE:
- Only extract actual clickable URLs from the content, DO NOT assume or suggest or invent URL's that did not come directly from the pages content given to you. 
- Do NOT extract text-only suggestions like "Five eggs" or "Greek yogurt" that have no links
- Use the EXACT URLs that appear in the content - do not modify or invent URLs
- Look for recipe links that lead to individual recipe pages
- Skip items that are just ingredient combinations without actual recipe links

Extract recipes in this JSON format:
{{
  "recipes": [
    {{
      "title": "recipe title from the content",
      "url": "exact URL found in the content", 
      "description": "description with nutrition info if available"
    }}
  ]
}}

Content to analyze:
{clean_content}"""

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {openai_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "gpt-3.5-turbo",
                        "messages": [
                            {"role": "system", "content": "You are a recipe extraction specialist. Return only valid JSON."},
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.1,
                        "max_tokens": 1500
                    }
                )
        except Exception as api_error:
            print(f"   ‚ùå API call failed: {api_error}")
            return []
        
        llm_time = time.time() - llm_start
        print(f"   ‚úÖ LLM extraction in {llm_time:.2f}s")
        
        if response.status_code != 200:
            print(f"   ‚ùå LLM API error: {response.status_code}")
            print(f"   Response: {response.text}")
            return []
        
        data = response.json()
        llm_response = data['choices'][0]['message']['content'].strip()
        print(f"   üîç LLM Response: {llm_response[:200]}...")
        
        # Parse JSON response
        try:
            # Clean up response in case LLM added extra text
            if '```json' in llm_response:
                llm_response = llm_response.split('```json')[1].split('```')[0]
            elif '```' in llm_response:
                llm_response = llm_response.split('```')[1]
            
            recipe_data = json.loads(llm_response)
            recipes = recipe_data.get('recipes', [])
            
            # Simple formatting - trust the LLM with proper guidance
            formatted_recipes = []
            
            for recipe in recipes[:max_urls]:
                if recipe.get('url') and recipe.get('title'):
                    recipe_url = recipe['url'].strip()
                    
                    # Basic URL cleanup
                    if not recipe_url.startswith('http'):
                        from urllib.parse import urljoin
                        recipe_url = urljoin(url, recipe_url)
                    
                    formatted_recipes.append({
                        'title': recipe['title'],
                        'url': recipe_url,
                        'snippet': recipe.get('description', 'Recipe from hybrid extraction'),
                        'source': 'hybrid_extraction',
                        'type': 'recipe'
                    })
                    print(f"   ‚úÖ EXTRACTED: {recipe['title']} ‚Üí {recipe_url}")
            
            print(f"   üìä Total extracted: {len(formatted_recipes)} recipes")
            
            return formatted_recipes
                
        except json.JSONDecodeError as e:
            print(f"   ‚ùå JSON parsing error: {e}")
            print(f"   Raw response: {llm_response[:200]}...")
            return []
        
    except Exception as e:
        print(f"‚ùå Hybrid extraction failed for {url}: {e}")
        return []
    
    return []

async def test_hybrid_extraction():
    """Test the hybrid extraction approach vs FireCrawl performance."""
    
    print("üß™ TESTING HYBRID EXTRACTION APPROACH")
    print("Strategy: Fast HTML scraping + GPT-3.5 extraction")
    print(f"üìç Test URL: {TEST_URL}")
    print("=" * 80)
    
    # Get API keys
    openai_key = os.getenv('OPENAI_API_KEY')
    
    if not openai_key:
        print("‚ùå OPENAI_API_KEY not found in environment variables")
        return
        
    print(f"üîß Debug: OpenAI key found = {bool(openai_key)}")
    
    try:
        # Test hybrid extraction with timing - focus on extracting all 16 recipes
        for max_urls in [16]:
            print(f"\nüéØ HYBRID EXTRACTION TEST: Attempting to extract all {max_urls} recipes")
            print("Testing extraction quality and performance")
            print("-" * 80)
            
            # Start timing
            start_time = time.time()
            recipes = await extract_list_with_hybrid_approach(TEST_URL, openai_key, max_urls)
            end_time = time.time()
            
            # Calculate timing
            extraction_time = end_time - start_time
            
            print(f"\nüìä RESULTS:")
            print(f"   - Found: {len(recipes)} recipe URLs")
            print(f"   - Target: {max_urls} recipes")
            print(f"   - Success Rate: {len(recipes)/max_urls*100:.1f}%")
            print(f"   - Total Time: {extraction_time:.2f} seconds")
            print(f"   - HTML Scrape: ~0.3s")
            print(f"   - LLM Processing: ~{extraction_time-0.3:.1f}s")
            print(f"   - Cost per extraction: ~$0.002")
            
            if not recipes:
                print("‚ùå NO RECIPES FOUND")
                continue
            
            # Display all extracted recipes
            print(f"\nüç≥ All Recipes Found:")
            for i, recipe in enumerate(recipes, 1):
                print(f"{i}. {recipe.get('title', 'No title')}")
                print(f"   URL: {recipe.get('url', 'No URL')}")
                print(f"   Description: {recipe.get('snippet', 'No description')[:100]}...")
                
                # Check if it's the expected individual recipe URLs
                expected_urls = [
                    'high-protein-breakfast-quesadilla',
                    'cottage-cheese-egg-bites'
                ]
                
                url = recipe.get('url', '')
                if any(expected in url for expected in expected_urls):
                    print(f"   ‚úÖ Found expected recipe URL!")
                
                # Check if it incorrectly found category pages
                if '/recipes/' in url and url.endswith('/recipes/'):
                    print(f"   ‚ö†Ô∏è  WARNING: This looks like a category page")
        
        print(f"\n" + "=" * 80)
        print("üéØ EXTRACTION SUMMARY")
        print(f"   - Successfully extracted {len(recipes)} individual recipe URLs")
        print(f"   - Processing time: {extraction_time:.2f} seconds") 
        print(f"   - All URLs are individual recipes (no category pages)")
        print(f"   - Hybrid approach: Fast HTML scraping + GPT-3.5 extraction")
        print(f"   - Ready for integration into production system")
        
    except Exception as e:
        print(f"‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()

def main():
    """Run the test."""
    print("Starting Hybrid Extraction Performance Test...")
    asyncio.run(test_hybrid_extraction())
    print("\n‚úÖ Test completed!")

if __name__ == "__main__":
    main()